# 排版技术笔记

# 0 杂感

可能接下来的很多篇都是杂感。

我们知道TeX的核心模型是box。但凡是排版，都会涉及到box，包括但不限于______（请自行填写）。如果看MathType，尤其是嵌入到doc文档中的公式，除了他的OLE属性之外，它的视觉效果本身就是个box。当然了，MathType在doc里面是怎么存的？WMF/EMF+它自己搞的一套数据结构（解析难度比CFF要简单一点）。

模型就是box，没有其他的东西了。至于你是一个线程充塞一个屏幕，还是多个线程搞版面雕花，那都是交互的事。不要搞混了。搞混了，写出来的东西就是灾难了。

我上周末说Qt 6的字体子集化效果好了一点。但确实只是好了一点。因为，Qt中做字体子集化，其中的办法很讨巧。用的是QPainterPath，然后封装成TrueType格式保存到PDF里面。好处是，确实像那么回事。坏处是，原始字体的很多信息肯定是丢了。聪明的读者自然想到，那CFF也被转换成TrueType了？是啊。三次贝塞尔曲线转二次贝塞尔曲线？是啊。用数段二次模拟一段三次。聪明的读者可能又想到：是不是hinting也丢了？是啊。当然了，很多字体没有hinting，那嵌入导出的PDF，最终生成的PDF文件，印出来会是什么鬼样。谁也不知道了。

Qt 6的QFont改进很多，可以配置variable font的属性，也可以配置shaping过程中开启的OpenType feature。好啊，排版精度高了不少呢。QFontDatabase的改进也不少，比如配置emoji字体之类。但QFontDatabase的问题是，注册字体的逻辑，其实文档是没描述清楚的。至于怎么不清楚，留与聪明的读者自行探索。

CFF2虽然常用于Variable Font，但本质上，是refinement（他们文档说的），去除了一些语焉不详的东西。如果不涉及到使用CFF2中的Variable Font相关操作，那么其实可以当作改良的CFF来用。当然了，CFF本身可以脱离于OpenType直接使用，那么CFF2必须要封装在一个OpenType的SFNT格式容器里了。所以理论上是会存在非可变的CFF2字体的。目前还没找到。但我自己创造了一个，这导致我和数位方家多年前写的工具崩了。崩归崩，修复是好修复的，改了两行。

用Qt来写代码，虽然很多人能写出来特别烂的代码。但是如果多读一点Qt本身的代码，就会很自然地跟着它那个代码风格来写。所以好代码是能激发出好代码的。当然不一定要特别好，优和良都可以嘛。

最近读MS-DOC，总感觉佶屈聱牙，或者像在英文中看abjad语言。这是因为里面很多都是匈牙利命名法。而Word的最初作者，就是匈牙利命名法的发明人。我连Word 97的格式文档都在看。里面很多缩写，我猜了个大概，写了一个术语对照表。但是呢，又想，前些年计算机博物馆不是发布了Word 1.1的源码了嘛，那就对照着看一看。你别说，你还真别说，有那味：这里面大部分缩写跟文档能对照上。而且越往后看，你就会越发现，后来的维护者采取了半匈牙利命名法，很多命名都越来越多的加上了整个单词来表达意义。这也说明，软件演进到一定程度了。

当然了，读过我写的文章的，知道有一篇Word编译的，可以对照看一看。即：
[编译MS Word 1.1a](https://zhuanlan.zhihu.com/p/48528499)。

又当然了，很多人肯定不看的，大概是：假装能看懂。探索未知的乐趣确实不是所有人都有的。

# 1 杂感

我最近脑子里面充满了回顾NTS烂活所得到的快乐和野望。NTS是什么呢？一个Java搞的下一代TeX系统。你们都没听说过？那自然，这东西烂尾了。当然，除了技术选型问题之外。排版软件，这么袖珍级别的，充满启发式思维的排版软件，都是需要理解很多材料的。我以前也讲过，排版的核心算法固然有难度，但有限。懒驴拉磨拉几年也都能懂了。但排版外的处理，要读的东西可就太多了。我觉得我现在也没读完。

快乐是：好像确实该把pTeX-ng的代码迁移到一个新形式的平台上了。是什么，无所谓，反正挖坑。重要的地方在于要把汉字排版的，特定于TeX的核心断行算法的部分，抽离出来。这就意味着我要写论文了。在写了在写了。

野望是，搞文档产品，总是有人用。我想想，可能结合我那个内部的字体软件，可以搞一个很原子化的，自由组合式的排版库。你什么牛档马档，最后落实，不都还是字符的二维有限空间组合？

最近偶尔也想，MathType如果有所更新，那么格式该是什么样子呢？好像是可以把Unicode Math和OpenType Math组合起来搞了。这……好像走得通，但是很多人搞不定这几个。（确实没几个人能搞定的）

昨天走路回窝。在想Unicode的Grapheme Cluster的匹配，是按文本的逻辑序来的。是不是可以搞一个逆形式？好像可以搞呢。

# 2 关于Digital Typography一书

上周买了一本Digital Typography。叫这名字的书有两本，一本是Richard Rubinstein写的，另一本是Knuth写的。这次买的是前者。后者我读过清华大学图书馆的副本，我也买了一个副本，还没到。

Typography有人翻译成排印学。我不大喜欢这个名字。毕竟现在这个时代，印刷和排版，已经是事实上的分离了的。所以叫做排版工艺是不错的。至于说“学”，其实对于很多人而言，是到不了这个高度的。我相信能进到这个领域的，很多人没有编程的技能，就算是遇到hyphenation算法，可能都要呲牙咧嘴。

这书里面有几处好玩的地方。

第一个是CRT上的bitmap字体绘制。这里讲了CRT的原理，加上人类视觉的局限（或特性），再加上bitmap字体，就构成了我们我们以前能够看到的文字。看完这个，我才明白现在一些复古游戏机模拟器上文字效果为什么跟以前的感觉不一样了。甚至可以推论，设备决定了字体的形态。

第二个是字体的制造方法。这个其实没有讲如何制造具体的字体。而是指制造字体的流程性方法。那么作者分了两种：自上而下的，自下而上的。这个所谓的上指的的outline，下指的是bitmap。所谓自下而上方法，就是先做最小的bitmap，最后做outline。这样能够保证在最局限的情况下的效果。

第三个是对于字体编程的看法。实话说，能拿得出手的字体编程工具，只有METAFONT。其他的，大多只存在于论文里或总有各种各样的毛病（比如说GlyphWiki的kage）。这东西真的太难了。无论是汉字还是西文，设计上虽然可以使用组件（component）来组合。但是能够以低成本且具有高描述性的语言来表示和生产，是很难的事。失败的例子很多。

第四个是教Typography的教程或者课程如何设计。这个很有意思。虽然只有两三页，但是真的要去做这种课程，讲一个月是可以了。实际上，这是作者给读者提供的一种元方法。

# 3 匠心不再独具

一般来说，夸人就可能用上匠心独具一词。这事吧，放在排版以及排版技术上说，已经很难这么讲了。

技术虽然进步，但是对于排版的细节的把握，其实是更差了。比如说，排版算法依赖程序所提供的。字体，靠着几大厂商的。但也就这，很多人还会纠结于很奇怪的问题，比如说拼音的字母形态。更别说在限定环境里充分利用已有的功能了。

据我所知，一些字体厂商其实不愿意做排版指导的（有越俎代庖之嫌）。但更多的厂商其实是不能。字体这事，现在想弄懂，实在是太难了。我这就积攒了相当多的地狱笑话。我都没想过一些厂商的技术人员的技术会有那么差。

字体方面的现状就是如此。虽然市面上有很多书，会讲一些所谓的排版知识。但这种书其实看了得不到什么收获。我经常揶揄：看完没记住就算学到了。更多的情况是很多人也不想系统了解这些平淡的知识，而喜欢闭门造车，自行理解。这个自我满足就够了，偏偏还喜欢说给别人，还振振有词。这可能就是一种平凡但清澈的愚蠢。

软件呢？就是提供给我们打包好了的排版算法的黑箱（有的是白箱）。所谓匠心，最多也就是软件制作者最初的匠心的直接副本。最近几年，有一些新软件出来。有些人就产生了“我用某软件，就代表我站在了对的方向”。这不对。无论用什么软件，最高哲学就是“定于一”。这个一就是它能不能帮“你”解决现实问题。这里面是两点，一个是应用者是自己，一个是作用范围是现实问题。

我之前也说，我喜欢关注现实问题。这跟我是个外行有关系：最初勾起我兴趣的就是直接的排版问题。我最近感觉我这外行搞得久了，好像也形成了一个认知链路，自成一系了。内行外行，如果都脱离了现实问题，那就往往会忘虚无的角度上跑。人一虚无，就要开始说胡话的。

但要说，真的有没有内行的且成体系的排版技术领域专著？我现在看不到。我自己了解到的东西就很分散。但因为接触过一系列奇怪的问题，形成了独特的直觉。这种专著太难写了，横跨领域包含但不限于：encoding/图形学/特定算法/语言特定知识。

# 4 看原型的重要性

想要了解一个东西，那么历史主义的视角是不可避免的。如果缺乏这种视角，那么往往会忽略掉要解决的问题的本质特征。

今天抽空看了Joe Becker的Multilingual Word Processing一文。Joe Becker没听说过？他是Unicode的奠基人之一。他在和其他人创建Unicode之前，一直从事计算机上的多语言环境的相关工作。

如果你了解Unicode，那么也需要看一看Multilingual Word Processing一文。当然了，如果不了解，那就更应该看了。这是一篇诞生在Unicode之前的文章。要解决的问题就是：如何把各种语言同时显示出来。80年代，大多数语言文字，其实都有了特定的编码系统。如果想要做一个编辑器，或者文本的查看器，那么随之而来的问题，就是如何把这些既有编码糅合到一起。这篇文章就给出了一个做法。虽然和Unicode不同，该文提出来的是一种糅合的协议。但从现实的角度上看，后来的Unicode实际上是在糅合的角度上做了更进一步。

所谓糅合，就是union。如果粗通一点翻译知识，那么大概就还会知道还有一种“协和”的翻译。实话说，Unicode这种东西，确实是搞多语言处理编辑的人能搞出来的东西。所以，现在的情况，Unicode也重要，非Unicode也同样重要的。所以有些人总觉得GB相关的编码里面是闭门造车，那也不对。

而编辑器的另一面呢？你显示文本就需要字体。那么多语言显示，背后就需要把字体糅合在一起。不过，现在还没有出现更进一步的字体技术。现有的技术，有时候64k的数量，已经足够的了。而且，从做字体的角度上看，一个字体往往也是往针对某个或者某几个block的unicode覆盖度去做。

# 5 PDF作为一种容器

总是说什么格式，似乎容易忽略format的本身含义。随便查一查，如OALD中对其解释为：*the way in which data is stored or held to be worked on by a computer*。那么用流行一点的词说，就是container。

PDF自然是一种容器。它看起来是一种静态化的结果，但将这种瞬时的静态组合起来就能变成动态。从这个角度看，PDF承载的对象就更直接了：某个排版软件的停机或者暂停的产物。

所以，能够将一个PDF中的页面画出来，多多少少就完成了排版软件前端的工作。这么一看，PDF阅读器的实现难度就可以转译为实现一个排版软件的部分难度上。拿我熟悉的东西来讲，PDF的难度，很大一部分来源于其层叠的不同时期的字体技术（最晚也就到2000年后）。

当然了，既然是继承自PostScript，那么就会有自举式的字体（Type 3）存在。Type 3中可以的对象：位图，矢量图，用某字体画的字符串，页面的一部分。这个就类似于TeX中的Virtual Font。之所以说是类似，是因为VF能放多少字符（或glyph）是几乎不受限制的，Unicode目前的设计上限是`0x10FFFF`，VF可以做到`0xFFFFFFFF`的。而Type 3只能做到`0xFF`。VF中自定义special，想画什么就画什么，驱动（driver）支持就行。

当然了，PDF文件内的内容是不是准确无误，这一点PDF的Specification是没说的。从实践上看，大部分PDF文件是不可信任的。不同的阅读器对于PDF的理解不同，报错/异常的行为不同，经验缺乏的情况下你实际上很难知道这个PDF是不是“对”的。文件内容有错，但渲染看不出来或者很难看出，这种情况非常常见。

PDF诞生的太早，为了compatibility，已然是个老登格式了。注意，我没说它过时。很多被认为“过时”的东西，你只能说它缺乏（匮乏甚至贫乏）相应的脚手架了。这种脚手架，往往包括文档，测试例以及一份能基本跑起来的代码（甚至包括运行时）。至于这种脚手架为什么缺乏，很多时候就是技术气候的变迁罢的一种结果。人对于价值的评价，如果从现在往未来看，那叫风险投资，如果从现在往过去看，那可能叫后悔。很多技术在当时的价值可能并不太高。

# 6 说点字体的事吧

最近工作上做了一点的小工作，能说的就是实时生成TrueType。在我的赛扬烂CPU上，1~2ms。在一些比较新的CPU上可能也就0.1ms左右。这个速度，我也还很满意。如果加上hinting，如利用`ttfautohint`，时间上可能就翻个倍而已。

我是很痛恨字体这些东西的。不会的时候，还有点雾里看花的美好。会了一点之后，就容易痛恨上。所以经常有人讲，不要把自己的爱好变成工作，否则会很痛苦的。一方面，你会发现某些技术文档，即使是公开的，也是有很多细节是找不到的。另一方面，你会发现很多业界产物不是按标准做的，或者说他们有一套“认为是对”的标准。比如说，有些字体乍一看你很难发现是对的还是错的。还有一点就是人，能不能坐下来听你说，也是很难的。

技术上的评判标准还好定。但另外的领域——设计——就容易陷入“自由心证”的状态。不是说不能这么干。心证也得有个公约数出来才行。没有公约数，协作就很难，出一系列标准物也是很难的。量化也不难做，难的是量化结果有了之后，如何执行。这是我最近思考的一个问题。

这就不得不提METAFONT和Computer Modern了。懂了METAFONT是如何工作的，那么看Computer Modern的设计就有了很浓重的醍醐味。但一般人看这个就有直接的两个难题：一是METAFONT看不懂，二是Computer Modern的设计参数过多。和拖控件设计外轮廓的字体设计软件来比，METAFONT确实不能直接给出视觉刺激，甚至依赖物理上的网格纸和坐标系。但这东西其实可以做一些GUI工具辅助的。这个没什么人做，我做了一点，自娱。后者呢，几十个参数多么？好像也不至于，毕竟英语字母还有26个呢。设计参数一分类，在上一点GUI手段，确实就直观了起来。

最近的余兴就是山寨Helvetica（使用METAFONT）。这东西，熟悉字体设计历史的人，知道重要性。但我以前一直有点无视的味道。最近能找到的一个比喻就是：返璞归真。把繁复的东西厘清并抽出最核心的东西，有时候人类就是要走一段弯路的。

另外，字体作为一种资源，现在就是跟Unicode牢牢地绑一起了。最近想了一些shaping相关的内容，发现以前的一些想法需要在做一点设计。这东西难，就是因为没什么直接的文档，需要利用现有的一些文档反复地查对并证明。想法落地，形式化很重要，公式形式的形式化，可以，图形式（状态机）会更好。

# 7 Hinting就是穷人的穷讲究

1977年，Knuth见到了在1 inch（2.54 cm）上能够打印出超过1000个点的印刷设备。后来做了METAFONT，目标也是直接输出bitmap，然后交给印刷/打印设备的。要知道，TeX诞生后一段时间内Adobe都还没成立。

如果以今天的角度看，TeX这套东西，讨论得多的是排版引擎，讨论的少的是METAFONT。比如，你当然可以批评METAFONT输出的bitmap在PDF上看起来不那么舒服。但你得要想一下，是不是自己有了更广泛的预设立场了。这就比如，看不透的时候的观点，大概可以总结为：这个圆不够方，或者这个方不够圆。我们都是学过一点欧几里得几何的人，是不是有一点荒诞感？

Hinting就是穷人的穷讲究。我之前就讲过，电脑上需要hinting是因为显示器的分辨率真的很低。用上高分屏，效果提升十分明显。但也不是所有地方能用上高分屏，比如我上班的地方就是俩大果粒屏幕，每天都用的很痛苦：因为我能看得见像素点。

用METAFONT能把书印好的，也得看印刷机。就算是Knuth的出版商Addison-Wesley这些年也是烂活频出，把Knuth的书印的乱七八糟，甚至有一批该销毁的还是被拿出来卖了。

这事我为什么知道呢，因为我有一年买了一套Computers & Typesetting，ABCDE，五卷印刷质量差异很大的。尤其是E卷，把Computer Modern的字都印丢了。我当时觉得这事很离谱啊，就直接找TUG的人反馈给Knuth了。确认是出版社搞烂活。

我手上还有一本活页本的The METAFONTbook，那效果……简直就是喷墨打印机打出来的。

为什么我这么感觉呢？这来源于我大四的时候，为了打论文，买了个佳能的喷墨打印机，200多。打印了大量的论文。后来同学也找我打论文，这东西买来就在给我搞回本了。我用这个打印机打过The Advanced TeXbook。那效果就是我手上这个The METAFONTbook的效果。熟悉喷墨打印机原理知道，民用的喷墨打印机分辨率很低的，打到纸上，墨点能渗透到纸上成什么鬼样子是没有确实的规律的。主打一个随意。

最近在推上冲浪，发现类似的问题日本人也发现了，OKI不同驱动版本出的文本效果不一样。不过我只看了一点，没追完全程。

讲这个什么意思呢？除了表达hinting是“穷讲究”之外。你想保证不同设备得到的输出都是自己满意的是不可能的。差异一定会有，只是某些场景下面可能你感觉不到。

Hinting的技术细节很复杂。但是大致流程就是告诉渲染设备，在什么情况下要把outline变形，然后那些像素点重要。现在的hinting很少有手动做的了，这就直接导致很多字体的hinting一旦实施，能多大程度地表达出原始设计，是很难说的。

还有一点，如果你碰到了那个字体或者typography爱好者写的东西云里雾里的，很多时候只会用“虚”来表述自己的感受，那你要注意了。他甚至不能准确表示出自己认为的“虚”是什么，那你就要对于他的观点有所保留。这情况很常见。他们可能觉得看了一些“权威”文章，好像自己也可以引用甚至复述了。

不能准确描述问题，就不能准确解决问题。

如果我输出的glyph是有此像素渲染的，有些人会觉得“虚”。如果我输出的glyph是灰度的，有些人会觉得“虚”。如果我输出的glyph是单色的，有些人会觉得“虚”。如果我输出的glyph是单色的，然后某些大聪明应用还加了滤镜（如preview），有些人也会觉得“虚”。

经常讲什么什么“虚”的，我印象里好像有个，叫中医。但中医这个呢，你黑不黑它，它这套系统里面的“虚”是分很多类的。

研究一个东西，想要取得确实的学问，重要的就是“做题”。要是不“做题”，就很容易变成物品收集，不知道什么能用什么不能用。堆积多了，跟在屋子里囤积物品也是类似的。
